# k8s 에 gpu 설정하기

- 기존 docker daemon 백업
```
$ cp /etc/docker/daemon.json /etc/docker/daemon.json.bak
```

- docker daemon 수정
```
$ vim /etc/docker/daemon.json

{
    "default-runtime": "nvidia",
    "runtimes": {
        "nvidia": {
            "path": "/usr/bin/nvidia-container-runtime",
            "runtimeArgs": []
        }
    }
}
```

- daemon reload
```
$ systemctl daemon-reload
$ systemctl restart docker
```

- device plugin enable 세팅
```
$ vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

아래 내용 추가

Environment="KUBELET_EXTRA_ARGS=--feature-gates=DevicePlugins=true"
```

- 데몬 적용
```
$ systemctl daemon-reload
$ systemctl restart kubelet
```

- k8s version 확인
```
$ kubectl version
```

아래 진행시 에러 발생
- version에 맞는 gpu support enable
```
$ $ kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/{version}/nvidia-device-plugin.yml

example
$ kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.12/nvidia-device-plugin.yml
```

- 변경 부분 적용
1. apps/v1으로 변경
2. 환경변수 추가
3. selector 추가

```
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-device-plugin-daemonset-1.12
  namespace: kube-system
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      name: nvidia-device-plugin-ds
  template:
    metadata:
      # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler
      # reserves resources for critical add-on pods so that they can be rescheduled after
      # a failure.  This annotation works in tandem with the toleration below.
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
      labels:
        name: nvidia-device-plugin-ds
    spec:
      tolerations:
      # Allow this pod to be rescheduled while the node is in "critical add-ons only" mode.
      # This, along with the annotation above marks this pod as a critical add-on.
      - key: CriticalAddonsOnly
        operator: Exists
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      containers:
      - image: nvidia/k8s-device-plugin:1.11
        name: nvidia-device-plugin-ctr
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        volumeMounts:
          - name: device-plugin
            mountPath: /var/lib/kubelet/device-plugins
      volumes:
        - name: device-plugin
          hostPath:
            path: /var/lib/kubelet/device-plugins
EOF
```

삭제는 아래와 같음
```
cat <<EOF | kubectl delete -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-device-plugin-daemonset-1.12
  namespace: kube-system
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      name: nvidia-device-plugin-ds
  template:
    metadata:
      # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler
      # reserves resources for critical add-on pods so that they can be rescheduled after
      # a failure.  This annotation works in tandem with the toleration below.
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
      labels:
        name: nvidia-device-plugin-ds
    spec:
      tolerations:
      # Allow this pod to be rescheduled while the node is in "critical add-ons only" mode.
      # This, along with the annotation above marks this pod as a critical add-on.
      - key: CriticalAddonsOnly
        operator: Exists
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      containers:
      - image: nvidia/k8s-device-plugin:1.11
        name: nvidia-device-plugin-ctr
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        volumeMounts:
          - name: device-plugin
            mountPath: /var/lib/kubelet/device-plugins
      volumes:
        - name: device-plugin
          hostPath:
            path: /var/lib/kubelet/device-plugins
EOF
```

- device-plugin 포드가 정상적으로 작동했는지 확인
```
$ kubectl -n kube-system get pod -l name=nvidia-device-plugin-ds

NAME                                        READY   STATUS    RESTARTS   AGE
nvidia-device-plugin-daemonset-1.12-7tvdl   1/1     Running   0          3m36s
nvidia-device-plugin-daemonset-1.12-xxthg   1/1     Running   0          3m36s
```

- 정상동작이 안된다면 로그 체크(하단은 정상동작 된 로그)
```
$ kubectl -n kube-system logs  -l name=nvidia-device-plugin-ds
```

- gpu 개수 확인

쿠버네티스가 사용 가능한 GPU 개수를 확인한다
```
$ kubectl get nodes "-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\.com/gpu"
```

- 테스트용으로 gpu 사용하는 파드 yaml
```
$ vim test-gpu.yaml

apiVersion: v1
kind: Pod
metadata:
  name: gpu-k8s
spec:
  containers:
  - name: gpu-container
    image: nvidia/cuda:9.0-runtime
    command:
      - "/bin/sh"
      - "-c"
    args:
      - nvidia-smi && tail -f /dev/null
    resources:
      requests:
        nvidia.com/gpu: 2
      limits:
        nvidia.com/gpu: 2
```

- yaml 적용
```
$ kubectl apply -f test-gpu.yaml
```

- 테스트용 pod 확인
```
$ kubectl get pod -o wide

NAME      READY   STATUS              RESTARTS   AGE   IP       NODE               NOMINATED NODE   READINESS GATES
gpu-k8s   0/1     ContainerCreating   0          36s   <none>   {}   <none>           <none>

시간이 지나면 running으로 변동(이미지 다운로드 시간때문에 늦음)
$ kubectl get pod
NAME      READY   STATUS    RESTARTS   AGE
gpu-k8s   1/1     Running   0          6m12s
```

- nvidia-smi 가 정상적으로 출력되었는지 확인

```
$ kubectl logs gpu-k8s

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |
| 23%   24C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  Off  | 00000000:09:00.0 Off |                  N/A |
| 23%   25C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```

- pod 삭제
```
$ kubectl delete -f test-gpu.yaml

pod "gpu-k8s" deleted

```

- gpu all pod 생성 yaml 작성
```
$ vim test-gpuall.yaml

apiVersion: v1
kind: Pod
metadata:
  name: gpu-all
spec:
  containers:
  - name: gpu-container
    image: nvidia/cuda:9.0-runtime
    command:
      - "/bin/sh"
      - "-c"
    args:
      - nvidia-smi && tail -f /dev/null
```

- yaml 적용
```
$ kubectl apply -f test-gpuall.yaml
```

- pod 확인
```
$ kubectl get pods

NAME      READY   STATUS    RESTARTS   AGE
gpu-all   1/1     Running   0          6s
```

- logs 확인
```
kubectl logs gpu-all
Thu Aug  6 03:45:31 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |
| 23%   25C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |
| 23%   24C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 108...  Off  | 00000000:09:00.0 Off |                  N/A |
| 23%   25C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 108...  Off  | 00000000:0A:00.0 Off |                  N/A |
| 23%   29C    P8     7W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  GeForce GTX 108...  Off  | 00000000:0B:00.0 Off |                  N/A |
| 23%   30C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   5  GeForce GTX 108...  Off  | 00000000:0C:00.0 Off |                  N/A |
| 23%   28C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   6  GeForce GTX 108...  Off  | 00000000:0D:00.0 Off |                  N/A |
| 23%   27C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  GeForce GTX 108...  Off  | 00000000:0E:00.0 Off |                  N/A |
| 23%   26C    P8     8W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   8  GeForce GTX 108...  Off  | 00000000:0F:00.0 Off |                  N/A |
| 23%   30C    P8     9W / 250W |      0MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
``` 


- pod 삭제

```
$ kubectl delete -f test-gpuall.yaml
```


## ref
- https://www.kangwoo.kr/2020/02/17/pc%EC%97%90-kubeflow-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0-2%EB%B6%80-kubernetes-nvidia-device-plugin-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0/
- https://medium.com/star-systems-labs/kubectl-convert-update-api-versions-automatically-e669add17e3d
- https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/