
# ref 
- https://medium.com/@Alibaba_Cloud/gpu-sharing-scheduler-extender-now-supports-fine-grained-kubernetes-clusters-b610b2c030b2

대부분 kubernetes 서비스(클라우드 공급업체 포함) 모두 NvidiaGPU 컨테이너를 예약하는 기능을 제공한다. 일반적으로는 컨테이너에 GPU Card를 할당하게 된다. 이는 격리를 가능하게 하고 GPU를 사용하는 애플리케이션이 다른 애플리케이션의 영향을 받지 않도록 한다. 딥 러닝 모델 학습 시나리오에는 적합하지만 모델 개발 및 모델 예측 시나리오에는 낭비가 된다. 수요는 더 많은 예측 서비스가 동일한 GPU 카드를 공유하도록 허용하여 클러스터에서 NvidiaGPU 사용률을 향상시키는 것이다. 이를 위해서는 GPU resource 분할 작업이 필요한다. 일반적으로 클러스터 수준 GPU 공유는 다음 두 가지에 관한것이다.

1. 스케줄링
2. 격리

주로 스케줄링에 대해서 설명하며, 격리 솔루션은 향후 Nvidia MPS를 기반으로 구현된다.

## 사용자 시나리오

- 클러스터 관리자로서 클러스터의 GPU 활용도를 높이고 싶다. 그리고 개발 중에 여러 사용자가 모델 개발 환경을 공유한다.

- 애플리케이션 개발자로서 Volta GPU에서 동시에 여러 논리 작업을 실행할 수 있다.

## 목표

- 사용자는 API를 통해 공유 리소스 신청을 하고 예약할 수 있다.

## 비 목표

- 공유 리소스의 격리는 지원하지 않는다.
- 초과 리소스 할당은 지원하지 않는다.

## 디자인 원리

- 문제는 명확히하고 디자인을 단순화한다. 첫 번째 단계에서는 예약 및 배포만 수행된 다음 런타임 메모리 제어가 구현된다.

많은 고객이 다중 AI 애플리케이션을 동일한 GPU로 예약할 수 있도록 허용해야하는 명확한 요구사항이 있다. 응용 프로그램 수준에서 메모리 크기 `gpu_options.per_process_gpu_memory_function`를 제어하고 응용 프로그램의 메모리 사용량을 제어하는 데 사용할 수 있다. 해결해야할 첫 번째 문제는 메모리 스케줄링 스케일로 사용하여 단순화하고, 메모리 크기를 매개 변수 형태로 컨테이너를 전송하는 것이다.

- intrusive 수정 없이 진행

해당 디자인에서는 확장 resource design, scheduler의 구현, 장치 plug-in의 메커니즘, kubelet의 관련 디자인과 같은 K8s design의 핵심을 수정하지 않는다. 확장 resource를 재사용하여 공유 resource에 대한 application API를 설명한다. 장점은 사용자가 기본 Kubernetes에서 사용할 수 있는 휴대용 솔루션을 제공하는 것이다.
